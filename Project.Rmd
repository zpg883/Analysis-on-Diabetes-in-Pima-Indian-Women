---
title: "STA 6543 Project"
author: "Hannah Haley"
date: '2022-07-24'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(AppliedPredictiveModeling); 
library(lattice); 
library(caret); 
library(kernlab)
library(earth); 
library(car); 
library(mlbench); 
library(corrplot)
library(e1071)
library(DataExplorer)
library(mice)
library(VIM)
#library(pls)
library(elasticnet)
library(randomForest)
library(MASS)
library(lars)

#for the missing values
library(dplyr)
library(naniar)
```

## Introduction and Background:

The objective of this analysis is to determine if a patient has diabetes by applying the statistical machine learning methods. The patient data can be found from the Pima Indians Diabetes Data.

The machine learning methods used to construct this analysis were:`K-nearest neighbors,`

## Data Structure:

The structure of the data consists of 768 observations and 9 variables. Out of those 9 variables, there is one target variable, `Outcome`, and the rest are predictors variancles which includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.

```{r, echo=FALSE}
library(readr)
diabetes <- read_csv("diabetes.csv")
diabetes <- data.frame(diabetes)
head(diabetes)
```

## Statistical Learning Methods
### Approach & Pre-processing

Before we could apply any methods to analyze our data, we needed to ensure that it was processed, divided into a training and testing set, determine if any values were missing, check for correlation, skewness, and potential outliers. 

```{r pressure, echo=FALSE}
#check for zero variance first
NZVdiabetes <- nearZeroVar(diabetes)
noNZVdiabetes <- diabetes[,-NZVdiabetes]
print(str(NZVdiabetes))
dim(noNZVdiabetes)

#partition
inOutcomeTraining <- createDataPartition(y = diabetes$Outcome, times = 1, p = 0.75, list = FALSE)

#training set
diabetesTrain <- diabetes[inOutcomeTraining,]

#testing set
diabetesTest <- diabetes[-inOutcomeTraining,]
```
**Observation:* There's no predictors with a zero variance. Also, as there are a small amount of observations, splitting the data 25/75 is a balanced approach for testing/training sets. 
```{r, echo=FALSE}
#check for correlation
corr <- cor(diabetesTrain)
corr
##correlation matrix
#corrplot::corrplot(corr, method = 'number', tl.cex = .35)

corrplot(corr, method = 'number', tl.cex = .35)
```
```{r, echo =FALSE}
#remove highly correlated
highcorr <- findCorrelation(corr, cutoff = .75)
head(highcorr)
#check skewness
##histograms
#outlier detection
##boxplots
```
**Observation:**
-Talk about the highly correlated predictors
-Observe that no predictor variables were removed from 0.75 cutoff. 



Analysis of missing values:
Insulin and SkinThickness are missing a significant number of observations.
```{r}
diabetes$Insulin <- na_if(diabetes$Insulin, 0)
diabetes$Glucose <- na_if(diabetes$Glucose, 0)
diabetes$BloodPressure <- na_if(diabetes$BloodPressure, 0)
diabetes$SkinThickness <- na_if(diabetes$SkinThickness, 0)
diabetes$BMI <- na_if(diabetes$BMI, 0)

```


```{r, warning=FALSE}
vis_miss(diabetes)
```


