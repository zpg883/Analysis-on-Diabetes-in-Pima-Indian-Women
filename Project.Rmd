
---
title: "STA 6543 Project"
author: "Abigail Dastur, Hannah Haley, Rachael Humphreys, and Karla Wiedmer"
date: '2022-07-28'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(AppliedPredictiveModeling); 
library(lattice); 
library(caret); 
library(kernlab)
library(earth); 
library(car); 
library(mlbench); 
library(corrplot)
library(e1071)
library(DataExplorer)
library(mice)
library(VIM)
#library(pls)
library(elasticnet)
library(randomForest)
library(MASS)
library(lars)

library(ggplot2)

#for the missing values
library(dplyr)
library(naniar)
```

## Introduction and Background:

The background of the study for the Pima Indian Dataset was conducted by scientists to analyze the significance of health related variables (predictors) to diabetes in Pima Indians Women. The samples' population was made up of females aged 21 years and older. Alongside, the population had an heritage of diabetes and digestive and kidney diseases.


The objective of this analysis is to determine if a patient has diabetes by applying various statistical machine learning methods. The patient data can be found from the Pima Indians Diabetes Data.

The machine learning methods used to construct this analysis were:`K-nearest neighbors,` `Logistic Regression,` `Random Forest,` and `Support Vector Machine`.

## Data Structure:

The structure of the data consists of 768 observations and 9 variables. Out of those 9 variables, there is one target variable, `Outcome`. The other eight variables are the predictor variables and includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on. Outcome was listed as 1 for a person with diabetes, and 0 for a person without diabetes. The data contains about twice as many examples of people without diabetes compared to people with diabetes.

```{r, include=FALSE}
diabetes <- read_csv("diabetes.csv")
diabetes <- data.frame(diabetes)

```

```{r, echo=FALSE}
head(diabetes)
```

```{r, echo=FALSE}
diabetes %>%
  group_by(Outcome) %>% 
  summarise(Outcome_n = n()) %>% 
  ggplot(aes(x = Outcome, y = Outcome_n)) +
  geom_col(aes(fill = Outcome)) +
  labs(x = "Diabetes", y = "") +
  theme(legend.position="none")

```


## Statistical Learning Methods
### Approach & Pre-processing

Before we could apply any methods to analyze our data, we needed to determine if there were any missing values, as well as check for correlation, skewness, and potential outliers. We also split the data into a training and testing set.

#### Near Zero Variance
```{r,  results='hide', echo=FALSE}
#I hid this output for now as I don't think it adds much visually in the report

#check for zero variance first
NZVdiabetes <- nearZeroVar(diabetes)
noNZVdiabetes <- diabetes[,-NZVdiabetes]
print(str(NZVdiabetes))
dim(noNZVdiabetes)

#partition
inOutcomeTraining <- createDataPartition(y = diabetes$Outcome, times = 1, p = 0.75, list = FALSE)

#training set
diabetesTrain <- diabetes[inOutcomeTraining,]

#testing set
diabetesTest <- diabetes[-inOutcomeTraining,]
```
**Observation:** It was observed that there are no predictors with near zero variance. Also, as there are a small amount of observations, splitting the data 25/75 is a balanced approach for testing/training sets.

#### Correlation
```{r, echo=FALSE}
#check for correlation
corr <- cor(diabetesTrain[-9])
corr
##correlation matrix
corrplot::corrplot(corr, method = 'number', tl.cex = .35)

#corrplot(corr, method = 'number', tl.cex = .35)
```
```{r, results='hide', echo=FALSE}
#remove highly correlated
highcorr <- findCorrelation(corr, cutoff = .75)
head(highcorr)
```
**Observation:**
While some correlation was observed there were no variables that were removed using a 0.75 cutoff. 

#### Missing Data
```{r, echo=FALSE}
#missing values next
colSums(diabetes[1:8] == 0)

```

**Analysis of missing values:**
```{r, echo=FALSE}
diabetes$Insulin <- na_if(diabetes$Insulin, 0)
diabetes$Glucose <- na_if(diabetes$Glucose, 0)
diabetes$BloodPressure <- na_if(diabetes$BloodPressure, 0)
diabetes$SkinThickness <- na_if(diabetes$SkinThickness, 0)
diabetes$BMI <- na_if(diabetes$BMI, 0)
```
```{r, warning=FALSE, echo=FALSE}
vis_miss(diabetes)
```
```{r,  results='hide', echo=FALSE}
#Also setting this to hide for now, as I think most of this was just doublechecking our solution.

#variables cannot be zero, so balance them out either by mean, median, etc. 
##Blood Pressure
meanBlood <- mean(diabetesTrain$BloodPressure[diabetesTrain$BloodPressure > 0])
diabetesTrain$BloodPressure<-ifelse(diabetesTrain$BloodPressure == 0, round(meanBlood,0), diabetesTrain$BloodPressure)

##Insulin
meanInsulin <-mean(diabetesTrain$Insulin[diabetesTrain$Insulin > 0])

diabetesTrain$Insulin <- ifelse(diabetesTrain$Insulin == 0, round(meanInsulin,0), diabetesTrain$Insulin)

##Skin Thickness
meanSkin <- mean(diabetesTrain$SkinThickness[diabetesTrain$Glucose > 0])
diabetesTrain$SkinThickness<-ifelse(diabetesTrain$SkinThickness == 0, round(meanSkin,0), diabetesTrain$Glucose)

##Glucose
meanGlucose <-mean(diabetesTrain$Glucose[diabetesTrain$Glucose > 0])
diabetesTrain$Glucose <-ifelse(diabetesTrain$Glucose == 0, round(meanGlucose,0), diabetesTrain$Glucose)
##BMI
meanBMI <- mean(diabetesTrain$BMI [diabetesTrain$BMI  > 0])
diabetesTrain$BMI<-ifelse(diabetesTrain$BMI  == 0, round(meanBMI ,0), diabetesTrain$BMI )

#data after treating missing values
summary(diabetesTrain)

#correlation after missing values
num_vars <- unlist(lapply(diabetesTrain, is.numeric))
dia_nums <- diabetesTrain[ , num_vars]
corr2 <- cor(dia_nums)
corrplot::corrplot(corr2, method = 'number', tl.cex = .35)
```

**Observation:**
Insulin and SkinThickness are missing a significant number of observations. Two approaches were tried. The first was replacing the missing values with the column mean, and the second was removing the Insulin and SkinThickness columns. Either approach gave similar model results so it was decided to move forward with replacing the missing values with column mean for all columns.

### Statistical Learning Methods & Models


```{r, result='hidden', echo=FALSE}
#Change Outcome to a factor.
diabetesTrain$Outcome <- as.factor(diabetesTrain$Outcome)
diabetesTest$Outcome <- as.factor(diabetesTest$Outcome)
```

```{r, result='hidden', echo=FALSE}
#Setup control for the models
set.seed(200)
indx <- createFolds(diabetesTrain$Outcome, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)
```

### K-Nearest Neighbor

The KNN model is good for the analysis because it is a good classifier for small datasets.

Model results using training data.

```{r, echo=FALSE}
set.seed(200)

knnGrid<- expand.grid(.k = c(3:10))

knn.model<- train(x = diabetesTrain[-9],
                 y = diabetesTrain$Outcome,
                method = "knn",
                tuneGrid = knnGrid,
                trControl = ctrl,
                tuneLength = 10,
                preProcess = c("center","scale"))

knn.model
```

Model results using test data.

```{r, echo=FALSE}
set.seed(100)
confusionMatrix(data = predict(knn.model, diabetesTest[-9]), reference = diabetesTest$Outcome)
```

**Observation:**

It was observed that the best k value was 9, which gave an Accuracy of 0.7640956 and a Kappa value of 0.4681284 on the training data. When the test data was used we saw an Accuracy of 0.7812 and a Kappa value of 0.4894.

### Support Vector Machines
Support Vector Machine is supervised learning and constructs hyper plane surfaces that predicts whether the examples fall into one class or another separated by a margin and classifies examples with the largest margin.

Model results using training data.

```{r, echo=FALSE}
set.seed(100)

#svmctrl <- trainControl(summaryFunction = defaultSummary)

sigmaRangeReduced <- sigest(as.matrix(diabetesTrain[-9]))
svmRGridReduced <- expand.grid(.sigma = sigmaRangeReduced[1],
                               .C = 2^(seq(-4, 4)))

SVMTune <- train(x = diabetesTrain[-9],
                 y = diabetesTrain$Outcome,
                 method = "svmRadial", 
                 metric = "Accuracy",
                 preProc = c("center", "scale"),
                 tuneGrid = svmRGridReduced,
                 fit = FALSE,
                 trControl = ctrl)
SVMTune

```

Model results using test data.
```{r, echo=FALSE}
set.seed(100)
confusionMatrix(data = predict(SVMTune, diabetesTest[-9]), reference = diabetesTest$Outcome)
```

**Observation:**
It was observed that the best C value was .5, which gave an Accuracy of 0.7638403 and a Kappa value of 0.446082093 on the training data. When the test data was used we saw an Accuracy of 0.7448 and a Kappa value of 0.3713.


### Random Forest
Random Forest is an extension of the decision tree and it produces different categories based on the predictors. This method is good for our analysis so we can categorize each health-related predictor.

Model results using training data.
```{r, echo=FALSE}
#indx <- createFolds(diabetesTrain$Outcome, returnTrain = TRUE)
#ctrl <- trainControl(method = "cv", index = indx)

mtryGrid <- data.frame(mtry = floor(seq(10, ncol(diabetesTrain[-9]), length = 10)))

ptm <- proc.time() #takes 520.62 seconds to run in my computer
### Tune the model using cross-validation
set.seed(100)
rf.fit <- train(x = diabetesTrain[-9],
                 y = diabetesTrain$Outcome,
                method = "rf",
                tuneGrid = mtryGrid,
                ntree = 200,
                importance = TRUE,
                trControl = ctrl)
rf.fit
proc.time() - ptm
```

Model results using test data.
```{r, echo=FALSE}
confusionMatrix(data = predict(rf.fit, diabetesTest[-9]), reference = diabetesTest$Outcome)
```

**Observation:**
It was observed that the best mtry value was 9, which gave an Accuracy of 0.7587114 and a Kappa value of 0.4490350 on the training data. When the test data was used we saw an Accuracy of 0.7865 and a Kappa value of 0.5108.


### Logistic Regression Model 
Logistic Regression model is good for our analysis because it is supervised learning which is used to calculate the prediction of a binary event occurring. This is perfect for our analysis to determine if a patient has diabetes in relation to other health issues.

Model results using training data.
```{r, echo=FALSE}
set.seed(1000)

#ctrl <- trainControl(summaryFunction = defaultSummary)

logisticTune  =  train(x = diabetesTrain[-9], 
                       y = diabetesTrain$Outcome, 
                       method = "multinom", 
                       metric = "Accuracy", 
                       trControl = ctrl)
logisticTune

plot(logisticTune, main = "Plot for shrinking threshold")
```

Model results using test data.
```{r, echo=FALSE}
confusionMatrix(data = predict(logisticTune, diabetesTest[-9]), reference = diabetesTest$Outcome)
```

**Observation:**
It was observed that the best decay value was 0.1, which gave an Accuracy of 0.7517995 and a Kappa value of 0.4199683 on the training data. When the test data was used we saw an Accuracy of 0.8385 and a Kappa value of 0.6217.

### Conclusion:

#### Summary of Test Results
Model | Accuracy | Kappa
------------- | ------------- | -------------
K Nearest | 0.7812 | 0.4894
SVM | 0.7448 | 0.3713
Random Forest | 0.7865 | 0.5108
Logistic Regression | 0.8385 | 0.6217




Checking out resamples just to see, we can delete this if we don't end up wanting to include it
```{r, echo=FALSE}
res = resamples(list(Logistic = logisticTune, SVM = SVMTune, RF=rf.fit, KNN=knn.model))
dotplot(res)
```

**Conclusion:**
In conclusion, the method that performs the best model was the Logistic Regression. As a result of Logistic Regression performing the best, it is a binary approach which works well with the Pima Indian dataset as it was a binary dataset. Our conclusion is based on the Kappa and Accuracy levels. 



























